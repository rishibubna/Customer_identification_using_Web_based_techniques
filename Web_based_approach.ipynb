{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.7/site-packages (1.0.8)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from langdetect) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating functions to clean tweets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import lang_detect\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    return \" \".join(stems)\n",
    "    lemmas = lemmatize_verbs(stems)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def only_english(mystr):\n",
    "    if lang_detect(mystr)=='en':   #give up\n",
    "        return mystr\n",
    "    else :\n",
    "        return ''\n",
    "\n",
    "\n",
    "def clean_string(mystr):\n",
    "    mystr=mystr.lower()\n",
    "    mystr=re.sub(r\"\\\\\\w+\", \" \", mystr)\n",
    "    mystr=re.sub(r\"\\@\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"\\#\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\" \",mystr)\n",
    "    \n",
    "    mystr=mystr[2:-1]\n",
    "    \n",
    "    cleantext = \"\".join([x.lower() if (x.isalpha() or x.isspace()) else ' ' for x in mystr])\n",
    "    return cleantext\n",
    "\n",
    "def remove_stop_words(mystr):\n",
    "    word_list= mystr.split()\n",
    "    cleaned_word_list=[w for w in word_list if w not in stop_words and len(w)>=2]\n",
    "    \n",
    "    return cleaned_word_list\n",
    "   \n",
    "    \n",
    "def join_sep(mylist):\n",
    "    return \" \".join(mylist)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impcolumns=list(vertical_stack.columns[np.argsort(chi2score[0])[::-1]][:1000])  #from a previous run, now i have cut those cells\n",
    "# #print(impcolumns)\n",
    "\n",
    "# with open(\"impcolumns.txt\", 'w') as f:\n",
    "#     for s in impcolumns:\n",
    "#         f.write(s + '\\n')\n",
    "\n",
    "#read these columns from a text file calledimpcolumns.txt if needed\n",
    "\n",
    "with open('impcolumns.txt', 'r') as f:\n",
    "    impcolumns = f.readlines()\n",
    "    \n",
    "\n",
    "    \n",
    "impcolumns = [line[:-1] for line in impcolumns]\n",
    "#print(impcolumns)\n",
    "vertical_stack=impcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading data and packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('approach2_final_data.json', 'r') as fp:\n",
    "    tweet_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using oversampled data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading Classifier and prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LinearSVC from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator BaggingClassifier from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                           fit_intercept=True,\n",
       "                                           intercept_scaling=1,\n",
       "                                           loss='squared_hinge', max_iter=1000,\n",
       "                                           multi_class='ovr', penalty='l2',\n",
       "                                           random_state=0, tol=1e-05,\n",
       "                                           verbose=0),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=50, n_jobs=None,\n",
       "                  oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkl_Filename='Classifiers/SVM_bagging.pkl'\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    classifier = clf = joblib.load(file)\n",
    "\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extras__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new.to_csv('X_train_oversampled.csv', index = False, header=True)\n",
    "# y_new.to_csv('y_train_oversampled.csv', index = False, header=True)\n",
    "# X_test.to_csv('X_test.csv', index = False, header=True)\n",
    "# y_test.to_csv('y_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making tfidf from tweets and score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_score(user_tweets):     #here user_tweets would be a dict with key as the username and value as the tweets \n",
    "\n",
    "     \n",
    "    dataframes = {}\n",
    "    dataframes_empty = {}\n",
    "\n",
    "\n",
    "    for x in user_tweets:\n",
    "    #print(x)\n",
    "        file=pd.DataFrame(user_tweets[x],columns=['id','text'])\n",
    "    \n",
    "        file['clean_text']=file['text'].apply(clean_string)\n",
    "        file['word_list']=file['clean_text'].apply(remove_stop_words)\n",
    "        file['word_list']=file['word_list'].apply(join_sep)\n",
    "        #file['word_list']=file['word_list'].apply(only_english)\n",
    "    \n",
    "        file=file.dropna()\n",
    "        file=file.loc[file['word_list']!='']\n",
    "    \n",
    "    \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "        \n",
    "            tfidfmatrix = vectorizer.fit_transform(file['word_list'])\n",
    "            vocab = vectorizer.get_feature_names()\n",
    "\n",
    "            tfidf_data=tfidfmatrix.toarray()\n",
    "        \n",
    "            tfidf_pd=pd.DataFrame(data=tfidf_data,columns=vocab,index=file['id'])\n",
    "        \n",
    "            tfidf_pd = tfidf_pd.drop(tfidf_pd.columns.difference(impcolumns),1)\n",
    "       \n",
    "        \n",
    "            dataframes[x] = tfidf_pd\n",
    "            #print(\"Success\",x)\n",
    "    \n",
    "        except ValueError as e: #happens when the vocabulary is empty\n",
    "        \n",
    "            #print(\"Failure\",x,file['text'])\n",
    "            dataframes_empty[x]='N/A'\n",
    "        \n",
    "        \n",
    "            continue\n",
    "            \n",
    "    tesla_followers=dataframes\n",
    "    feature_list=impcolumns\n",
    "    #UNIFORM PADDING\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "    \n",
    "        #print(len(test.columns))\n",
    "    \n",
    "\n",
    "        toadd=list(set(feature_list)-set(test.columns))\n",
    "\n",
    "\n",
    "        toadd_df=pd.DataFrame(np.zeros((test.shape[0], len(toadd))),columns=toadd)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        toadd_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        test=pd.concat([test,toadd_df],axis=1)\n",
    "    \n",
    "        tesla_followers[key]=test\n",
    "        #print(test.shape)\n",
    "\n",
    "    \n",
    "    #SCORE    \n",
    "    tesla_tweet_prediction=[]\n",
    "    score={}\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "        current_user_pred=classifier.predict(test)\n",
    "        tesla_tweet_prediction.append(current_user_pred)\n",
    "        score[key]=sum(current_user_pred)/len(current_user_pred)   \n",
    "    \n",
    "\n",
    "    import collections\n",
    "    from collections import OrderedDict\n",
    "    from operator import itemgetter \n",
    "    sorted_score = OrderedDict(sorted(score.items(), key=itemgetter(1),reverse=True))\n",
    "    sorted_score.update(dataframes_empty)    #adding users with No tweets\n",
    "    \n",
    "    return sorted_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level={}\n",
    "\n",
    "for k, v in tweet_data['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    first_level[k]=v['tweets']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('mahamunidurai', 0.5294117647058824), ('iamdarkwing', 0.5), ('KedriW', 0.5), ('Alex62249401', 0.48148148148148145), ('Angelin57066746', 0.4675324675324675), ('Opsilon_Y', 0.4444444444444444), ('BRITTNEY0623', 0.4), ('bait4love', 0.3230769230769231), ('qnguyvn', 0.30434782608695654), ('makoma_pulcia', 0.26666666666666666), ('Prabhas27919923', 0.2631578947368421), ('Nickie52611168', 0.2222222222222222), ('UnicornModest', 0.2), ('ercanyildirimtr', 0.1377551020408163), ('nook2ladiez', 0.0), ('tiberiu99783268', 0.0), ('MarkWel72212278', 0.0), ('AnnaSamkov1', 0.0), ('abdoabeed17', 0.0), ('JojoWest8', 0.0), ('Allan_Cortez_V', 0.0), ('koukoucang', 0.0), ('Thierno79594590', 'N/A'), ('Rooney05091', 'N/A'), ('Rahim42420139', 'N/A'), ('Sarvino46081636', 'N/A'), ('MiikaTolonen5', 'N/A'), ('BEATA18510687', 'N/A'), ('LukasPidgeon', 'N/A'), ('vicman7721', 'N/A'), ('Ajaycha44323532', 'N/A'), ('ANXDRXN1', 'N/A'), ('AbuHuzain', 'N/A'), ('ZtofKr', 'N/A'), ('Niveen50330395', 'N/A'), ('Gerardo46481687', 'N/A'), ('lemonposting', 'N/A'), ('Petr62448205', 'N/A'), ('CeramicsCms', 'N/A'), ('PechtRabassa', 'N/A'), ('LordBennu', 'N/A'), ('JoshTD_', 'N/A'), ('G61Tony', 'N/A'), ('GogicK03', 'N/A'), ('clinton_soares', 'N/A'), ('Julius182020', 'N/A'), ('janani2021', 'N/A'), ('DaSausi', 'N/A'), ('MantuDoru', 'N/A')])\n"
     ]
    }
   ],
   "source": [
    "print(return_score(first_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level={}\n",
    "for k, v in tweet_data['tesla'].items():\n",
    "    second_level[k] = {}\n",
    "    follwer_list = []\n",
    "    user_tweets={}\n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            \n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                    follwer_list.append(i)\n",
    "                    if j['tweets']!=\"error\":\n",
    "                        user_tweets[i] = j['tweets']\n",
    "    second_level[k] = return_score(user_tweets)\n",
    "                        \n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "with open('hierarchy_scores.json', 'w') as fp:\n",
    "    json.dump(second_level, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMEANS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_avg(scores_list):\n",
    "    X = np.array(scores_list)\n",
    "    if len(scores_list)>5:\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0).fit(X.reshape(-1, 1))\n",
    "        label = kmeans.labels_\n",
    "        counts = np.bincount(label)\n",
    "        cluster_no = (np.argmax(counts))\n",
    "        result = np.array(list(zip(X,label)))\n",
    "        res = result[result[:,1] == cluster_no]\n",
    "        web_score = np.mean(res[:,0])\n",
    "    else:\n",
    "        #print(scores_list)\n",
    "        web_score=np.mean(scores_list)\n",
    "    \n",
    "    return web_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "second_level_list={}\n",
    "for x,y in second_level.items():\n",
    "    try:\n",
    "        if not(y.values()==[] or y.values()==['N/A']):\n",
    "            second_level_list[x]=kmeans_avg(list(y.values()))\n",
    "        else:\n",
    "            second_level_list[x]='N/A(web)'\n",
    "    except:\n",
    "        second_level_list[x]='N/A(web)'\n",
    "    \n",
    "    \n",
    "    \n",
    "print(len(second_level_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n"
     ]
    }
   ],
   "source": [
    "ctr=0\n",
    "for k,v in tweet_data['tesla'].items():\n",
    "    #print(k)\n",
    "    \n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                #print(i)\n",
    "                ctr+=1\n",
    "                \n",
    "                \n",
    "print(ctr)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_file = open(\"sorted_score_first_level.csv\", \"w\")\n",
    "# #a_dict = {\"a\": 1, \"b\": 2}\n",
    "# import csv\n",
    "\n",
    "# writer = csv.writer(a_file)\n",
    "# for key, value in sorted_score.items():\n",
    "#     writer.writerow([key, value])\n",
    "\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
