{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.7/site-packages (1.0.8)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from langdetect) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating functions to clean tweets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import lang_detect\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    return \" \".join(stems)\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# def stem_and_lemmatize(words):\n",
    "#     stems = stem_words(words)\n",
    "#     return \" \".join(stems)\n",
    "#     lemmas = lemmatize_verbs(stems)\n",
    "#     return \" \".join(lemmas)\n",
    "\n",
    "def only_english(mystr):\n",
    "    if lang_detect(mystr)=='en':   #give up\n",
    "        return mystr\n",
    "    else :\n",
    "        return ''\n",
    "\n",
    "\n",
    "def clean_string(mystr):\n",
    "    mystr=mystr.lower()\n",
    "    mystr=re.sub(r\"\\\\\\w+\", \" \", mystr)\n",
    "    mystr=re.sub(r\"\\@\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"\\#\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\" \",mystr)\n",
    "    \n",
    "    mystr=mystr[2:-1]\n",
    "    \n",
    "    cleantext = \"\".join([x.lower() if (x.isalpha() or x.isspace()) else ' ' for x in mystr])\n",
    "    return cleantext\n",
    "\n",
    "def remove_stop_words(mystr):\n",
    "    word_list= mystr.split()\n",
    "    cleaned_word_list=[w for w in word_list if w not in stop_words and len(w)>=2]\n",
    "    \n",
    "    return cleaned_word_list\n",
    "   \n",
    "    \n",
    "def join_sep(mylist):\n",
    "    return \" \".join(mylist)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important columns stored after feature selection in impcolumns.txt\n",
    "\n",
    "with open('impcolumns.txt', 'r') as f:\n",
    "    impcolumns = f.readlines()\n",
    "    \n",
    "\n",
    "    \n",
    "impcolumns = [line[:-1] for line in impcolumns]\n",
    "#print(impcolumns)\n",
    "vertical_stack=impcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading data and packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f= approach_FORGRAPH_tweets_data.json'\n",
    "import json\n",
    "with open('approach4_final_data.json', 'r') as fp:\n",
    "    tweet_data0 = json.load(fp)\n",
    "    \n",
    "    \n",
    "with open('approach2_FORGRAPH1.json', 'r') as fp:\n",
    "    tweet_data1 = json.load(fp)\n",
    "    \n",
    "with open('approach2_FORGRAPH2.json', 'r') as fp:\n",
    "    tweet_data2 = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using oversampled data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading Classifier and prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.22.2.post1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                         class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=None,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort='deprecated',\n",
       "                                                         random_state=None,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pkl_Filename='Classifiers/DT_adaboost.pkl'\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    classifier = clf = joblib.load(file)\n",
    "\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extras__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new.to_csv('X_train_oversampled.csv', index = False, header=True)\n",
    "# y_new.to_csv('y_train_oversampled.csv', index = False, header=True)\n",
    "# X_test.to_csv('X_test.csv', index = False, header=True)\n",
    "# y_test.to_csv('y_test.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making tfidf from tweets and score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_score(user_tweets):     #here user_tweets would be a dict with key as the username and value as the tweets \n",
    "\n",
    "     \n",
    "    dataframes = {}\n",
    "    dataframes_empty = {}\n",
    "\n",
    "\n",
    "    for x in user_tweets:\n",
    "    #print(x)\n",
    "        \n",
    "    \n",
    "        try:\n",
    "            file=pd.DataFrame(user_tweets[x],columns=['id','text'])\n",
    "    \n",
    "            file['clean_text']=file['text'].apply(clean_string)\n",
    "            file['word_list']=file['clean_text'].apply(remove_stop_words)\n",
    "            file['word_list']=file['word_list'].apply(stem_and_lemmatize)\n",
    "            #file['word_list']=file['word_list'].apply(join_sep)\n",
    "            #file['word_list']=file['word_list'].apply(only_english)\n",
    "    \n",
    "            file=file.dropna()\n",
    "            file=file.loc[file['word_list']!='']\n",
    "            \n",
    "            ##############\n",
    "    \n",
    "    \n",
    "            vectorizer = TfidfVectorizer()\n",
    "        \n",
    "            tfidfmatrix = vectorizer.fit_transform(file['word_list'])\n",
    "            vocab = vectorizer.get_feature_names()\n",
    "\n",
    "            tfidf_data=tfidfmatrix.toarray()\n",
    "        \n",
    "            tfidf_pd=pd.DataFrame(data=tfidf_data,columns=vocab,index=file['id'])\n",
    "        \n",
    "            tfidf_pd = tfidf_pd.drop(tfidf_pd.columns.difference(impcolumns),1)\n",
    "       \n",
    "        \n",
    "            dataframes[x] = tfidf_pd\n",
    "            #print(\"Success\",x)\n",
    "    \n",
    "        except ValueError as e: #happens when the vocabulary is empty\n",
    "        \n",
    "            #print(\"Failure\",x,file['text'])\n",
    "            dataframes_empty[x]='N/A'\n",
    "        \n",
    "        \n",
    "            continue\n",
    "            \n",
    "    tesla_followers=dataframes\n",
    "    feature_list=impcolumns\n",
    "    #UNIFORM PADDING\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "    \n",
    "        #print(len(test.columns))\n",
    "    \n",
    "\n",
    "        toadd=list(set(feature_list)-set(test.columns))\n",
    "\n",
    "\n",
    "        toadd_df=pd.DataFrame(np.zeros((test.shape[0], len(toadd))),columns=toadd)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        toadd_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        test=pd.concat([test,toadd_df],axis=1)\n",
    "    \n",
    "        tesla_followers[key]=test\n",
    "        #print(test.shape)\n",
    "\n",
    "    \n",
    "    #SCORE    \n",
    "    tesla_tweet_prediction=[]\n",
    "    score={}\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "        current_user_pred=classifier.predict(test)\n",
    "        tesla_tweet_prediction.append(current_user_pred)\n",
    "        score[key]=sum(current_user_pred)/len(current_user_pred)   \n",
    "    \n",
    "\n",
    "    import collections\n",
    "    from collections import OrderedDict\n",
    "    from operator import itemgetter \n",
    "    sorted_score = OrderedDict(sorted(score.items(), key=itemgetter(1),reverse=True))\n",
    "    sorted_score.update(dataframes_empty)    #adding users with No tweets\n",
    "    \n",
    "    return sorted_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n"
     ]
    }
   ],
   "source": [
    "first_level={}\n",
    "first_no_of_tweets={}\n",
    "f_list=[]\n",
    "\n",
    "ctr=0\n",
    "\n",
    "\n",
    "for k, v in tweet_data0['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    if k not in first_level:\n",
    "        first_level[k]=v['tweets']\n",
    "        f_list.append(k)\n",
    "        first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "for k, v in tweet_data1['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    if k not in first_level:\n",
    "        first_level[k]=v['tweets']\n",
    "        f_list.append(k)\n",
    "        first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "for k, v in tweet_data2['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    if k not in first_level:\n",
    "        first_level[k]=v['tweets']\n",
    "        f_list.append(k)\n",
    "        first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "\n",
    "    \n",
    "print(len(first_no_of_tweets))\n",
    "#print(f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level={}\n",
    "\n",
    "for k, v in tweet_data['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    first_level[k]=v['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_level_list= return_score(first_level)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level={}\n",
    "for k, v in tweet_data['tesla'].items():\n",
    "    second_level[k] = {}\n",
    "    follwer_list = []\n",
    "    user_tweets={}\n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            \n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                    follwer_list.append(i)\n",
    "                    if j['tweets']!=\"error\":\n",
    "                        user_tweets[i] = j['tweets']\n",
    "    second_level[k] = return_score(user_tweets)\n",
    "                        \n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2903\n"
     ]
    }
   ],
   "source": [
    "g=0\n",
    "ctr=0\n",
    "for x in second_level:\n",
    "    \n",
    "    #print(len(second_level[x]))\n",
    "    g+=len(second_level[x])\n",
    "    \n",
    "    \n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "with open('hierarchy_scores.json', 'w') as fp:\n",
    "    json.dump(second_level, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMEANS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_avg(scores_list):\n",
    "    X = np.array(scores_list)\n",
    "    if len(scores_list)>5:\n",
    "        kmeans = KMeans(n_clusters=3, random_state=0).fit(X.reshape(-1, 1))\n",
    "        label = kmeans.labels_\n",
    "        counts = np.bincount(label)\n",
    "        cluster_no = (np.argmax(counts))\n",
    "        result = np.array(list(zip(X,label)))\n",
    "        res = result[result[:,1] == cluster_no]\n",
    "        web_score = np.mean(res[:,0])\n",
    "    else:\n",
    "        #print(scores_list)\n",
    "        web_score=np.mean(scores_list)\n",
    "    \n",
    "    return web_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kenziemelonn': 0.3519369246262211, 'AguonJojo': nan, 'orkunalbayrakk': nan, 'chuanlongwu': nan, 'stahl_waylon': nan, 'LightTheoryLLC': 'N/A(web)', 'TonyLGM': 0.32463090612219025, 'MdSiddi2': nan, 'TheBuffBengali': 0.26877030630922916, 'kutbuImus': 'N/A(web)', 'rongron22700921': 0.42908697221219777, 'akash2396': nan, 'Surajit31177051': 0.41145833333333337, 'SkiljoDenis': nan, 'JeevaVj96231590': 0.23537581699346405, 'MarkinhoMaciel': 'N/A(web)', 'clot503': nan, 'Yhimna1': nan, 'cObW3Etwih6JjOl': 0.507433489827856, 'StartAHealtyLif': 0.5403783440418197, 'RitejGaba': 0.5113271019912662, 'Netassa2': 0.3787878787878788, 'satyamjigurjar': nan, 'AmirRakim': nan, 'willthompson94': 0.31080920845651383, 'JoshuaWinkelma5': 0.4665158371040724, 'AfxcSack': 0.298196574100233, 'jason71805778': nan, 'geewu': 'N/A(web)', 'SrikarKolli': 0.43915343915343913, 'ShawnStack1': nan, '_Arnab_': 0.33250412510099053, 'yophooey': 0.35255824577432615, '14_rjay': 'N/A(web)', 'nadhirahbaharin': 0.3623077765190306, 'lane71183': 0.3994998866986423, '3ryvn': 0.3246999006238395, 'gabrunu': 0.3669900392014167, 'txeddierubio': nan, 'nucara_demetrio': nan, 'Overlor08302263': nan, 'Dominic78515025': nan, 'Yang32073409': nan, 'BurnettDavison': 0.3445575651434331, 'ahmedtarek686': 0.2873987050960735, 'Dantruckerman': 0.38658586406283424, 'BraffCatieli': nan, 'sherafatia': 'N/A(web)', 'weez_mc': 0.33369006331305207, 'MikeFoleyJr': 0.2991236372188302, 'Nasser31866880': 'N/A(web)', 'AshwinK40032796': nan, 'wenbeing': nan, 'AhmedBehram': 'N/A(web)', 'santigrrpo': nan, 'GenjiPhilip': 0.4123684210526316, 'therealpythonix': nan, 'SaifKhazal1': nan, 'arunbala28': 0.3185903044170761, 'luciabbott15': 0.4971782535573455, 'mlesikar15': 0.3225572132402228, 'euorafah': 'N/A(web)', 'JimBain1': 0.44105739891030454, 'krcollins04': 0.32857142857142857, 'shmoon0212': 0.37802161843217114, 'yusufom08169211': nan, 'RogueBeast1': nan, 'shm_mike': 0.37940185560995726, 'Mukesh94483394': 0.5211390337706128, 'AzBridgette': nan, 'Sickyman678': nan, 'JoshuaD96008981': 0.8592964824120602, 'captmayfieldbtc': 'N/A(web)', 'Adriano_BNA': 0.344668048283886, 'cd_tan': 0.6186145346366341, 'AvichalDm': nan, 'Maxxx_Dodds': 0.2811307289212449, 'Vipal96395134': 0.6594594594594595, 'lutti000': 'N/A(web)', 'TwoDumbBitches2': nan, 'miriam87an': 0.40059781488352914, 'Joshavez': 0.32044198895027626, 'UkaegbuJonathan': 0.3195993355090207, 'jackie_cheatham': 0.32663316582914576, 'Brandon_Kolb': 'N/A(web)', 'solarpowergroup': 'N/A(web)', 'PeppersSoggy': 0.41397849462365593, 'Lc67390288': nan, 'HongWeiyou': nan, 'MypadZtrk': 'N/A(web)', 'bhawkinsdpm': 0.37879603980051685, 'snowwolf32008': nan, 'jerome_sri': 0.4765417868162064, 'brattila03': 0.37744354566902594, 'JoeCZhou': 0.3402527404493973, 'jgerdtsl': 0.3567486597287922, 'johnjonny21': 0.3236994219653179, 'DeveshT87659238': 'N/A(web)', 'wdmateus': nan, 'SeanWeller17': 0.35320588436464795, 'Griny85296791': nan, 'Stefanie_CH1986': 1.0, 'JMivillanueva': 'N/A(web)', 'tw1stwf': 'N/A(web)', 'travislemoine': 0.5214236219246341, 'MehmetYldrm': 'N/A(web)', 'jogidas_': 0.2702282157676349, '2pandaadrish': nan, 'LeoNeo1990099': nan, 'RyanSul31437054': nan, 'Omar37930430': 0.6428571428571428, 'kirti_amrit': nan, 'workdesign88': nan, 'SheTimeTea': 0.40414507772020725, 'AbdelsalamAlab1': 'N/A(web)', 'AkhileshNirmam': 0.4563758389261745, 'LemmyWalls': 0.30588009194125554, 'Savageasfu': nan, 'gamalito17': 0.435392238972641, 'whakhi_ice': 0.3625140492386919, 'oldair35371939': nan, 'andy_dyble': nan, 'DefenceSky': 0.2, 'Dhiraj_kuppili': nan, 'totocaeltimbre': 'N/A(web)', 'Rishi_Jay_Patil': nan, '0xwal2_': nan, 'CarlosAlbarrn11': nan, 'i3bady_aljh': 'N/A(web)', '_aaroncarver': 0.40766552568272557, 'defmass': nan, 'gqGp4L0g35NcGkc': 'N/A(web)', 'kanhaiyachauha': 0.3858189740542682, 'AudiSolis1': 0.3312883435582822, 'Butterflydemoc': nan, 'ThokozaniTsoari': 0.311294585313314, 'buraakattes': 'N/A(web)', 'Guaropapi': 0.3520100089961436, 'naturalcity': 0.3534167052011962, 'Troy93571982': 0.4562334217506631, 'RayzerOptical': 0.40045720620068503, 'kaiqueks02': 'N/A(web)', 'ColbyJ57741451': nan, 'raterik510': 0.37585320072156403, 'ActorChadJones': 'N/A(web)', 'Ghepetto51': 0.3960601453742615, 'p_e_r_v_e_t_t_e': 0.32626115375892323, 'ngenoutlaw': 0.31976744186046513, 'SueJungie': 0.3094292862472086, 'houdrhiri': 0.40468765468765466, 'haidong1013': nan, 'whxstellar': 0.49652063213345043, 'jajajagun2016': nan, 'LucasM86Arg': nan, 'joebobjames': nan, 'loki_rivers': nan, 'CummingsX42': 0.5305858631822915, 'PhilpSiobhan': nan, 'JackieeXP': 'N/A(web)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "second_level_list={}\n",
    "\n",
    "for x,y in second_level.items():\n",
    "    try:\n",
    "        if not(y.values()==[] or y.values()==['N/A']):\n",
    "            second_level_list[x]=kmeans_avg(list(y.values()))\n",
    "        else:\n",
    "            second_level_list[x]='N/A(web)'\n",
    "    except:\n",
    "        second_level_list[x]='N/A(web)'\n",
    "    \n",
    "    \n",
    "    \n",
    "print(second_level_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison={}\n",
    "\n",
    "for x in first_level:\n",
    "    comparison[x]=list([first_no_of_tweets[x],first_level_list[x],second_level_list[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42  5 20  5]\n"
     ]
    }
   ],
   "source": [
    "#print(comparison)\n",
    "\n",
    "threshold=0.5\n",
    "\n",
    "comp_pd=pd.DataFrame.from_dict(data=comparison)\n",
    "comp_pd=comp_pd.transpose()\n",
    "comp_pd.columns=['#Tweets','Text based','Web based'] \n",
    "comp_pd = comp_pd[comp_pd['Text based'].apply(lambda x: isinstance(x, float) )]\n",
    "comp_pd = comp_pd[comp_pd['Web based'].apply(lambda x: isinstance(x, float)   )]\n",
    "comp_pd = comp_pd.dropna()\n",
    "\n",
    "comp_pd.loc[(comp_pd['Text based'] < threshold),'T Result']='No'\n",
    "comp_pd.loc[(comp_pd['Text based'] >= threshold),'T Result']='Yes'\n",
    "comp_pd.loc[(comp_pd['Web based'] < threshold),'W Result']='No'\n",
    "comp_pd.loc[(comp_pd['Web based'] >= threshold),'W Result']='Yes'\n",
    "\n",
    "comp_pd.loc[(comp_pd['Text based'] < threshold) & (comp_pd['Web based'] < threshold) , 'Match'] = 'Yes'\n",
    "comp_pd.loc[(comp_pd['Text based'] >= threshold) & (comp_pd['Web based'] >= threshold) , 'Match'] = 'Yes'\n",
    "comp_pd.loc[(comp_pd['Text based'] >= threshold) & (comp_pd['Web based'] < threshold) , 'Match'] = 'No'\n",
    "comp_pd.loc[(comp_pd['Text based'] < threshold) & (comp_pd['Web based'] >= threshold) , 'Match'] = 'No'\n",
    "\n",
    "comp_pd['Text based']=comp_pd['Text based'].apply(lambda x: round(x,3))\n",
    "comp_pd['Web based']=comp_pd['Web based'].apply(lambda x: round(x,3))\n",
    "\n",
    "#print(comp_pd)\n",
    "comp_pd.to_csv('comparison.csv',encoding='utf-8')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true=list(comp_pd['T Result'])\n",
    "y_pred=list(comp_pd['W Result'])\n",
    "\n",
    "print(confusion_matrix(y_true,y_pred).ravel())  # Confusion matrix printed\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
