{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/umasreeram/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def remove_stop_words(mystr):\n",
    "    word_list= mystr.split()\n",
    "    cleaned_word_list=[w for w in word_list if w not in stop_words and len(w)>=2]\n",
    "    \n",
    "    return cleaned_word_list\n",
    "   \n",
    "    \n",
    "def join_list(wordlist):\n",
    "    \n",
    "    return \" \".join(wordlist)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_ebay_tweets.csv\n",
      "clean_ladygaga_tweets.csv\n",
      "clean_premierleague_tweets.csv\n",
      "clean_tesla_tweets.csv\n",
      "clean_FoodandTravelEd_tweets.csv\n",
      "clean_facebook_tweets.csv\n",
      "clean_usedgov_tweets.csv\n",
      "clean_MTV_tweets.csv\n",
      "clean_parenting_tweets.csv\n",
      "clean_nytimes_tweets.csv\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "goal_dir = os.path.join(os.getcwd(), \"cleaned_csv/\")\n",
    "\n",
    "\n",
    "ctr=0\n",
    "for filename in os.listdir(goal_dir):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        print(filename)\n",
    "        ctr+=1\n",
    "            \n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk_dict = nltk.help.upenn_tagset()\n",
    "#print(type(nltk_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_ebay_tweets.csv Done\n",
      ".DS_Store Done\n",
      "clean_ladygaga_tweets.csv Done\n",
      "clean_premierleague_tweets.csv Done\n",
      "clean_tesla_tweets.csv Done\n",
      "clean_FoodandTravelEd_tweets.csv Done\n",
      "clean_facebook_tweets.csv Done\n",
      "clean_usedgov_tweets.csv Done\n",
      "clean_MTV_tweets.csv Done\n",
      "clean_parenting_tweets.csv Done\n",
      "clean_nytimes_tweets.csv Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#NN,NNP,NNPS,NNS\n",
    "\n",
    "goal_dir = os.path.join(os.getcwd(), \"cleaned_csv/\")\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "vocab_pos={}\n",
    "\n",
    "import os\n",
    "\n",
    "def pos_tagger(sent):\n",
    "    \n",
    "    sent=nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "def return_only_nouns(wordlist):\n",
    "    nouns=[]\n",
    "    \n",
    "    for x in wordlist:\n",
    "        if x[1]=='NN' or x[1]=='NNP' or x[1]=='NNPS' or x[1]=='NNS':\n",
    "            nouns.append(x[0])\n",
    "            \n",
    "    return nouns\n",
    "\n",
    "def join_sep(mylist):\n",
    "    return \" \".join(mylist)\n",
    "    \n",
    "\n",
    "pos_vocab={}\n",
    "total_vocab=[]\n",
    "    \n",
    "for filename in os.listdir(goal_dir):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        file = pd.read_csv(\"cleaned_csv/\"+filename)\n",
    "        #print(type(file['cleaned_word_list'].iloc[1]))\n",
    "        \n",
    "        file = file[file['word list'].notnull()]\n",
    "        \n",
    "        file['all_words']=file['word list'].apply(lambda x:x.split())\n",
    "        \n",
    "\n",
    "        file['pos_tags_all_words']=file['all_words'].apply(pos_tagger)  \n",
    "        \n",
    "        file['nouns']=file['pos_tags_all_words'].apply(return_only_nouns)\n",
    "        \n",
    "        file['cleaned_word_list_nouns']=file['nouns'].apply(join_list)\n",
    "        \n",
    "        file['x']=file['all_words'].apply(join_sep)\n",
    "        \n",
    "#         vectorizer = TfidfVectorizer()\n",
    "        \n",
    "        \n",
    "#         tfidfmatrix = vectorizer.fit_transform(file['x'])\n",
    "\n",
    "#         vocab = vectorizer.get_feature_names()\n",
    "#         tfidf_data=tfidfmatrix.toarray()\n",
    "        \n",
    "#         pos_vocab[filename]=pos_tagger(vocab)\n",
    "#         total_vocab.extend(pos_tagger(vocab))\n",
    "    \n",
    "        \n",
    "        del file['x']\n",
    "        del file['created_at']\n",
    "        del file['text']\n",
    "        del file['clean_text']\n",
    "        \n",
    "        del file['pos_tags_all_words']\n",
    "        del file['nouns']\n",
    "        del file['all_words']\n",
    "        \n",
    "        file.to_csv(r'pos_csv/pos_'+filename, encoding='utf-8')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    print(filename,\"Done\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean_ebay_tweets.csv': Counter({'NN': 1974, 'JJ': 1050, 'NNS': 815, 'VBP': 443, 'VBG': 351, 'VBD': 245, 'RB': 200, 'VBN': 118, 'VBZ': 89, 'VB': 56, 'IN': 50, 'JJS': 28, 'FW': 21, 'JJR': 17, 'CD': 11, 'MD': 8, 'RBR': 6, 'NNP': 6, 'CC': 4, 'DT': 3, 'PRP': 3, 'RP': 2, 'WP': 1, 'UH': 1, 'WDT': 1, 'WP$': 1, 'WRB': 1}), 'clean_ladygaga_tweets.csv': Counter({'NN': 1977, 'JJ': 957, 'NNS': 593, 'VBP': 353, 'VBG': 302, 'VBD': 238, 'RB': 181, 'VBN': 91, 'VBZ': 84, 'IN': 56, 'VB': 36, 'NNP': 29, 'JJS': 28, 'FW': 23, 'JJR': 18, 'RBR': 12, 'CD': 10, 'MD': 9, 'WP': 4, 'DT': 3, 'PRP': 3, 'CC': 2, 'RBS': 1, 'RP': 1, 'WDT': 1, 'WP$': 1}), 'clean_premierleague_tweets.csv': Counter({'NN': 1425, 'JJ': 713, 'NNS': 438, 'VBP': 236, 'VBG': 189, 'VBD': 168, 'RB': 142, 'VBZ': 66, 'VBN': 66, 'IN': 48, 'VB': 32, 'FW': 25, 'JJS': 20, 'NNP': 19, 'JJR': 13, 'CD': 11, 'RBR': 10, 'MD': 7, 'DT': 6, 'RP': 3, 'RBS': 1, 'WP': 1, 'UH': 1, 'CC': 1, 'PRP': 1, 'WP$': 1}), 'clean_tesla_tweets.csv': Counter({'NN': 1748, 'JJ': 883, 'NNS': 581, 'VBP': 300, 'VBG': 276, 'VBD': 211, 'RB': 192, 'VBN': 99, 'VBZ': 76, 'IN': 51, 'VB': 46, 'NNP': 32, 'JJS': 27, 'FW': 20, 'JJR': 14, 'CD': 13, 'RBR': 8, 'MD': 7, 'DT': 3, 'PRP': 3, 'CC': 2, 'WP': 2, 'WRB': 2, 'TO': 1}), 'clean_FoodandTravelEd_tweets.csv': Counter({'NN': 2534, 'JJ': 1348, 'NNS': 856, 'VBP': 494, 'VBG': 348, 'VBD': 299, 'RB': 229, 'VBZ': 124, 'VBN': 121, 'IN': 66, 'VB': 47, 'FW': 27, 'JJS': 26, 'NNP': 22, 'JJR': 19, 'CD': 12, 'RBR': 9, 'MD': 7, 'DT': 4, 'WRB': 3, 'RP': 2, 'WP': 2, 'CC': 2, 'PRP': 2, 'WDT': 2, 'TO': 1, 'WP$': 1}), 'clean_facebook_tweets.csv': Counter({'NN': 1674, 'JJ': 706, 'NNS': 338, 'VBP': 200, 'VBG': 197, 'VBD': 131, 'RB': 126, 'VBN': 70, 'VBZ': 59, 'VB': 40, 'FW': 40, 'IN': 36, 'NNP': 32, 'CD': 10, 'JJS': 8, 'MD': 8, 'JJR': 5, 'RBR': 5, 'CC': 5, 'PRP': 3, 'DT': 2, 'RBS': 1, 'RP': 1, 'WP': 1, 'UH': 1, 'WP$': 1}), 'clean_usedgov_tweets.csv': Counter({'NN': 1907, 'JJ': 1076, 'NNS': 759, 'VBP': 385, 'VBG': 353, 'VBD': 236, 'RB': 198, 'VBN': 124, 'VBZ': 108, 'IN': 48, 'VB': 44, 'FW': 24, 'JJS': 23, 'JJR': 15, 'NNP': 13, 'CD': 12, 'MD': 8, 'RBR': 6, 'DT': 3, 'CC': 3, 'RP': 2, 'PRP$': 2, 'WP': 2, 'TO': 1, 'PRP': 1, 'WRB': 1, 'WDT': 1, 'WP$': 1}), 'clean_MTV_tweets.csv': Counter({'NN': 1554, 'JJ': 786, 'NNS': 447, 'VBG': 258, 'VBP': 254, 'VBD': 196, 'RB': 172, 'VBN': 72, 'VBZ': 71, 'IN': 41, 'VB': 26, 'JJS': 21, 'NNP': 15, 'CD': 11, 'JJR': 8, 'FW': 8, 'MD': 6, 'DT': 4, 'RBR': 4, 'CC': 2, 'WP': 2, 'PRP': 2, 'WRB': 2, 'RBS': 1, 'RP': 1, 'UH': 1, 'WDT': 1}), 'clean_parenting_tweets.csv': Counter({'NN': 686, 'JJ': 389, 'NNS': 333, 'VBP': 170, 'VBG': 116, 'RB': 72, 'VBD': 61, 'VBZ': 39, 'VBN': 33, 'VB': 23, 'IN': 23, 'FW': 11, 'NNP': 9, 'JJS': 8, 'JJR': 6, 'CD': 6, 'MD': 5, 'RBR': 4, 'DT': 2, 'CC': 2, 'PRP': 2, 'RBS': 1, 'WDT': 1}), 'clean_nytimes_tweets.csv': Counter({'NN': 3695, 'JJ': 2013, 'NNS': 1442, 'VBP': 717, 'VBG': 640, 'VBD': 628, 'RB': 370, 'VBN': 273, 'VBZ': 145, 'IN': 84, 'VB': 67, 'FW': 60, 'JJS': 44, 'JJR': 38, 'NNP': 32, 'CD': 18, 'RBR': 12, 'MD': 9, 'RP': 6, 'CC': 6, 'RBS': 3, 'DT': 2, 'WP': 2, 'PRP': 2, 'PRP$': 1, 'UH': 1, 'WDT': 1, 'WRB': 1, 'WP$': 1})}\n"
     ]
    }
   ],
   "source": [
    "#print(pos_vocab)\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def pos_distribution(pos_vocab):\n",
    "    distribution={}\n",
    "    \n",
    "    for x in pos_vocab:\n",
    "        tags = pos_vocab[x]\n",
    "        counts = Counter( tag for word,  tag in tags)\n",
    "        distribution[x]=counts\n",
    "    \n",
    "    return distribution\n",
    "\n",
    "print(pos_distribution(pos_vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun\n",
    "#Pronoun\n",
    "#Verb\n",
    "#Adjective\n",
    "#Adverb\n",
    "#Preposition\n",
    "#Conjunction\n",
    "#Interjection\n",
    "# import numpy as np\n",
    "\n",
    "# parts_of_speech=pd.DataFrame((total_counts),index=np.arange(1,27))\n",
    "# parts_of_speech=parts_of_speech.transpose()\n",
    "# parts_of_speech=parts_of_speech[1]\n",
    "\n",
    "# print(parts_of_speech[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
