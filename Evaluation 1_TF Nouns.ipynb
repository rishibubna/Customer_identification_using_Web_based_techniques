{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# goal_dir = os.path.join(os.getcwd(), \"pos_csv/\")\n",
    "\n",
    "# combination=[]\n",
    "\n",
    "\n",
    "# ebay_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_eBay_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_eBay_tweets_drop2.csv\")])\n",
    "# ladygaga_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_ladygaga_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_ladygaga_tweets_drop2.csv\")])\n",
    "# premierleague_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_premierleague_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_premierleague_tweets_drop2.csv\")])\n",
    "# parenting_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_parenting_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_parenting_tweets_drop2.csv\")])\n",
    "# FoodandTravelEd_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_FoodandTravelEd_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_FoodandTravelEd_tweets_drop2.csv\")])\n",
    "# usedgov_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_usedgov_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_usedgov_tweets_drop2.csv\")])\n",
    "# facebook_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_facebook_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_facebook_tweets_drop2.csv\")])\n",
    "# nytimes_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_nytimes_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_nytimes_tweets_drop2.csv\")])\n",
    "# tesla_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_tesla_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_tesla_tweets_drop2.csv\")])\n",
    "# MTV_tweets=pd.concat([pd.read_csv(\"pos_csv/\"+\"pos_clean_MTV_tweets.csv\"),pd.read_csv(\"pos_csv/\"+\"pos_clean_MTV_tweets_drop2.csv\")])\n",
    "\n",
    "\n",
    "# ebay_tweets.to_csv(r'new_files/'+'ebay_tweets'+'.csv',encoding='utf-8')\n",
    "# ladygaga_tweets.to_csv(r'new_files/'+'ladygaga_tweets'+'.csv', encoding='utf-8')\n",
    "# premierleague_tweets.to_csv(r'new_files/'+'premierleague_tweets'+'.csv', encoding='utf-8')\n",
    "# parenting_tweets.to_csv(r'new_files/'+'parenting_tweets'+'.csv', encoding='utf-8')\n",
    "# FoodandTravelEd_tweets.to_csv(r'new_files/'+'FoodandTravelEd_tweets'+'.csv', encoding='utf-8')\n",
    "# usedgov_tweets.to_csv(r'new_files/'+'usedgov_tweets'+'.csv', encoding='utf-8')\n",
    "# facebook_tweets.to_csv(r'new_files/'+'facebook_tweets'+'.csv', encoding='utf-8')\n",
    "# nytimes_tweets.to_csv(r'new_files/'+'nytimes_tweets'+'.csv', encoding='utf-8')\n",
    "# tesla_tweets.to_csv(r'new_files/'+'tesla_tweets'+'.csv', encoding='utf-8')\n",
    "# MTV_tweets.to_csv(r'new_files/'+'MTV_tweets'+'.csv', encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# goal_dir = os.path.join(os.getcwd(), \"tweets_raw/\")\n",
    "\n",
    "# combination=[]\n",
    "\n",
    "\n",
    "# ebay_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"eBay_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"eBay_tweets_drop2.csv\")])\n",
    "# ladygaga_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"ladygaga_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"ladygaga_tweets_drop2.csv\")])\n",
    "# premierleague_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"premierleague_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"premierleague_tweets_drop2.csv\")])\n",
    "# parenting_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"parenting_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"parenting_tweets_drop2.csv\")])\n",
    "# FoodandTravelEd_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"FoodandTravelEd_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"FoodandTravelEd_tweets_drop2.csv\")])\n",
    "# usedgov_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"usedgov_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"usedgov_tweets_drop2.csv\")])\n",
    "# facebook_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"facebook_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"facebook_tweets_drop2.csv\")])\n",
    "# nytimes_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"nytimes_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"nytimes_tweets_drop2.csv\")])\n",
    "# tesla_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"tesla_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"tesla_tweets_drop2.csv\")])\n",
    "# MTV_tweets=pd.concat([pd.read_csv(\"tweets_raw/\"+\"MTV_tweets.csv\"),pd.read_csv(\"tweets_raw/\"+\"MTV_tweets_drop2.csv\")])\n",
    "\n",
    "\n",
    "# ebay_tweets.to_csv(r'new_files2/'+'ebay_tweets'+'.csv',encoding='utf-8')\n",
    "# ladygaga_tweets.to_csv(r'new_files2/'+'ladygaga_tweets'+'.csv', encoding='utf-8')\n",
    "# premierleague_tweets.to_csv(r'new_files2/'+'premierleague_tweets'+'.csv', encoding='utf-8')\n",
    "# parenting_tweets.to_csv(r'new_files2/'+'parenting_tweets'+'.csv', encoding='utf-8')\n",
    "# FoodandTravelEd_tweets.to_csv(r'new_files2/'+'FoodandTravelEd_tweets'+'.csv', encoding='utf-8')\n",
    "# usedgov_tweets.to_csv(r'new_files2/'+'usedgov_tweets'+'.csv', encoding='utf-8')\n",
    "# facebook_tweets.to_csv(r'new_files2/'+'facebook_tweets'+'.csv', encoding='utf-8')\n",
    "# nytimes_tweets.to_csv(r'new_files2/'+'nytimes_tweets'+'.csv', encoding='utf-8')\n",
    "# tesla_tweets.to_csv(r'new_files2/'+'tesla_tweets'+'.csv', encoding='utf-8')\n",
    "# MTV_tweets.to_csv(r'new_files2/'+'MTV_tweets'+'.csv', encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOPIC MODELLING RESULTS\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_clean_parenting_tweets.csv done\n",
      "pos_clean_usedgov_tweets.csv done\n",
      "pos_clean_facebook_tweets.csv done\n",
      "pos_clean_nytimes_tweets.csv done\n",
      "pos_clean_ebay_tweets.csv done\n",
      "pos_clean_tesla_tweets.csv done\n",
      "pos_clean_MTV_tweets.csv done\n",
      "pos_clean_FoodandTravelEd_tweets.csv done\n",
      "pos_clean_premierleague_tweets.csv done\n",
      "pos_clean_ladygaga_tweets.csv done\n"
     ]
    }
   ],
   "source": [
    "#TERM FREQUENCY COUNTS\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "goal_dir = os.path.join(os.getcwd(), \"pos_csv/\")\n",
    "\n",
    "\n",
    "tf={}\n",
    "tm={}\n",
    "\n",
    "for filename in os.listdir(goal_dir):\n",
    "    if filename.endswith(\".csv\"): \n",
    "        file = pd.read_csv(\"pos_csv/\"+filename)\n",
    "        file=file[file['cleaned_word_list_nouns'].notnull()]\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        cvmatrix = vectorizer.fit_transform(file['cleaned_word_list_nouns'])\n",
    "\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "        cv_data=cvmatrix.toarray().sum(axis=0)\n",
    "        \n",
    "        #print(type(vocab))\n",
    "        #print((cv_data))\n",
    "        cv_pd=pd.DataFrame(list(zip(vocab, cv_data)), columns =['Term', 'frequency']) \n",
    "        cv_pd= cv_pd.sort_values(by='frequency', ascending=False)\n",
    "        #print(cv_pd.head())\n",
    "        tf[filename[10:]]=list(cv_pd[:10]['Term'])\n",
    "\n",
    "        cv_pd.to_csv(r'terms_frequency_nouns/'+filename, encoding='utf-8')\n",
    "        \n",
    "        \n",
    "        \n",
    "        number_of_topics = 1\n",
    "        model = LatentDirichletAllocation(n_components=number_of_topics, random_state=45) # random state for reproducibility\n",
    "        # Fit data to model\n",
    "        model.fit(cvmatrix)\n",
    "        df= display_topics(model,vocab,10)\n",
    "        df.columns=['words','weights']\n",
    "        \n",
    "        tm[filename[10:]]=list(df[:10]['words'])\n",
    "        \n",
    "        df.to_csv(r'topic_modelling_nouns/'+filename, encoding='utf-8')\n",
    "        \n",
    "        print(filename,\"done\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parenting_tweets.csv': ['kids', 'baby', 'sale', 'gifts', 'day', 'fun', 'ideas', 'toys', 'co', 'family'], 'usedgov_tweets.csv': ['rt', 'students', 'school', 'education', 'today', 'student', 'teachers', 'schools', 'amp', 'college'], 'facebook_tweets.csv': ['hi', 'account', 'facebook', 'kn', 'thanks', 'page', 'help', 'please', 'visit', 'report'], 'nytimes_tweets.csv': ['coronavirus', 'rt', 'president', 'news', 'trump', 'people', 'state', 'york', 'co', 'day'], 'ebay_tweets.csv': ['rt', 'help', 'ebay', 'shop', 'time', 'deals', 'day', 'today', 'items', 'thanks'], 'tesla_tweets.csv': ['model', 'rt', 'tesla', 'car', 'thanks', 'co', 'drive', 'owners', 'years', 'thank'], 'MTV_tweets.csv': ['rt', 'video', 'day', 'yes', 'love', 'episode', 'song', 'time', 'heart', 'today'], 'FoodandTravelEd_tweets.csv': ['year', 'rt', 'food', 'travel', 'awards', 'vote', 'co', 'issue', 'thanks', 'hotel'], 'premierleague_tweets.csv': ['rt', 'goal', 'time', 'mins', 'man', 'goals', 'team', 'birthday', 'city', 'half'], 'ladygaga_tweets.csv': ['rt', 'gaga', 'lady', 'amp', 'thank', 'love', 'today', 'time', 'day', 'world']}\n",
      "                                      0         1          2          3  \\\n",
      "parenting_tweets.csv               kids      baby       sale      gifts   \n",
      "usedgov_tweets.csv                   rt  students     school  education   \n",
      "facebook_tweets.csv                  hi   account   facebook         kn   \n",
      "nytimes_tweets.csv          coronavirus        rt  president       news   \n",
      "ebay_tweets.csv                      rt      help       ebay       shop   \n",
      "tesla_tweets.csv                  model        rt      tesla        car   \n",
      "MTV_tweets.csv                       rt     video        day        yes   \n",
      "FoodandTravelEd_tweets.csv         year        rt       food     travel   \n",
      "premierleague_tweets.csv             rt      goal       time       mins   \n",
      "ladygaga_tweets.csv                  rt      gaga       lady        amp   \n",
      "\n",
      "                                 4        5         6         7       8  \\\n",
      "parenting_tweets.csv           day      fun     ideas      toys      co   \n",
      "usedgov_tweets.csv           today  student  teachers   schools     amp   \n",
      "facebook_tweets.csv         thanks     page      help    please   visit   \n",
      "nytimes_tweets.csv           trump   people     state      york      co   \n",
      "ebay_tweets.csv               time    deals       day     today   items   \n",
      "tesla_tweets.csv            thanks       co     drive    owners   years   \n",
      "MTV_tweets.csv                love  episode      song      time   heart   \n",
      "FoodandTravelEd_tweets.csv  awards     vote        co     issue  thanks   \n",
      "premierleague_tweets.csv       man    goals      team  birthday    city   \n",
      "ladygaga_tweets.csv          thank     love     today      time     day   \n",
      "\n",
      "                                  9  \n",
      "parenting_tweets.csv         family  \n",
      "usedgov_tweets.csv          college  \n",
      "facebook_tweets.csv          report  \n",
      "nytimes_tweets.csv              day  \n",
      "ebay_tweets.csv              thanks  \n",
      "tesla_tweets.csv              thank  \n",
      "MTV_tweets.csv                today  \n",
      "FoodandTravelEd_tweets.csv    hotel  \n",
      "premierleague_tweets.csv       half  \n",
      "ladygaga_tweets.csv           world  \n"
     ]
    }
   ],
   "source": [
    "print(tm)\n",
    "top10=pd.DataFrame(tm)\n",
    "top10=top10.transpose()\n",
    "print(top10)\n",
    "top10.columns = ['0', '1', '2','3','4','5','6','7','8','9']\n",
    "\n",
    "top10['vocabulary']=top10['0']+' ,'+top10['1']+' ,'+top10['2']+' ,'+top10['3']+' ,'+top10['4']+' ,'+top10['5']+' ,'+top10['6']+' ,'+top10['7']+' ,'+top10['8']+' ,'+top10['9']\n",
    "del top10['0']\n",
    "del top10['1']\n",
    "del top10['2']\n",
    "del top10['3']\n",
    "del top10['4']\n",
    "del top10['5']\n",
    "del top10['6']\n",
    "del top10['7']\n",
    "del top10['8']\n",
    "del top10['9']\n",
    "top10.to_csv(r'topic_modelling_nouns/'+\"top10.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parenting_tweets.csv': ['kids', 'baby', 'sale', 'gifts', 'day', 'fun', 'ideas', 'co', 'toys', 'family'], 'usedgov_tweets.csv': ['rt', 'students', 'school', 'education', 'today', 'student', 'teachers', 'schools', 'amp', 'college'], 'facebook_tweets.csv': ['hi', 'account', 'facebook', 'kn', 'thanks', 'page', 'help', 'please', 'visit', 'report'], 'nytimes_tweets.csv': ['coronavirus', 'rt', 'president', 'news', 'trump', 'people', 'state', 'york', 'co', 'day'], 'ebay_tweets.csv': ['rt', 'help', 'ebay', 'shop', 'time', 'deals', 'day', 'today', 'items', 'thanks'], 'tesla_tweets.csv': ['model', 'rt', 'tesla', 'car', 'thanks', 'co', 'drive', 'owners', 'years', 'thank'], 'MTV_tweets.csv': ['rt', 'video', 'day', 'yes', 'love', 'episode', 'song', 'time', 'heart', 'today'], 'FoodandTravelEd_tweets.csv': ['year', 'rt', 'food', 'travel', 'awards', 'vote', 'co', 'issue', 'thanks', 'hotel'], 'premierleague_tweets.csv': ['rt', 'goal', 'time', 'mins', 'man', 'goals', 'team', 'birthday', 'city', 'half'], 'ladygaga_tweets.csv': ['rt', 'gaga', 'lady', 'amp', 'thank', 'love', 'today', 'time', 'day', 'world']}\n",
      "                                      0         1          2          3  \\\n",
      "parenting_tweets.csv               kids      baby       sale      gifts   \n",
      "usedgov_tweets.csv                   rt  students     school  education   \n",
      "facebook_tweets.csv                  hi   account   facebook         kn   \n",
      "nytimes_tweets.csv          coronavirus        rt  president       news   \n",
      "ebay_tweets.csv                      rt      help       ebay       shop   \n",
      "tesla_tweets.csv                  model        rt      tesla        car   \n",
      "MTV_tweets.csv                       rt     video        day        yes   \n",
      "FoodandTravelEd_tweets.csv         year        rt       food     travel   \n",
      "premierleague_tweets.csv             rt      goal       time       mins   \n",
      "ladygaga_tweets.csv                  rt      gaga       lady        amp   \n",
      "\n",
      "                                 4        5         6         7       8  \\\n",
      "parenting_tweets.csv           day      fun     ideas        co    toys   \n",
      "usedgov_tweets.csv           today  student  teachers   schools     amp   \n",
      "facebook_tweets.csv         thanks     page      help    please   visit   \n",
      "nytimes_tweets.csv           trump   people     state      york      co   \n",
      "ebay_tweets.csv               time    deals       day     today   items   \n",
      "tesla_tweets.csv            thanks       co     drive    owners   years   \n",
      "MTV_tweets.csv                love  episode      song      time   heart   \n",
      "FoodandTravelEd_tweets.csv  awards     vote        co     issue  thanks   \n",
      "premierleague_tweets.csv       man    goals      team  birthday    city   \n",
      "ladygaga_tweets.csv          thank     love     today      time     day   \n",
      "\n",
      "                                  9  \n",
      "parenting_tweets.csv         family  \n",
      "usedgov_tweets.csv          college  \n",
      "facebook_tweets.csv          report  \n",
      "nytimes_tweets.csv              day  \n",
      "ebay_tweets.csv              thanks  \n",
      "tesla_tweets.csv              thank  \n",
      "MTV_tweets.csv                today  \n",
      "FoodandTravelEd_tweets.csv    hotel  \n",
      "premierleague_tweets.csv       half  \n",
      "ladygaga_tweets.csv           world  \n"
     ]
    }
   ],
   "source": [
    "print(tf)\n",
    "top10=pd.DataFrame(tf)\n",
    "top10=top10.transpose()\n",
    "print(top10)\n",
    "top10.columns = ['0', '1', '2','3','4','5','6','7','8','9']\n",
    "\n",
    "top10['vocabulary']=top10['0']+' ,'+top10['1']+' ,'+top10['2']+' ,'+top10['3']+' ,'+top10['4']+' ,'+top10['5']+' ,'+top10['6']+' ,'+top10['7']+' ,'+top10['8']+' ,'+top10['9']\n",
    "del top10['0']\n",
    "del top10['1']\n",
    "del top10['2']\n",
    "del top10['3']\n",
    "del top10['4']\n",
    "del top10['5']\n",
    "del top10['6']\n",
    "del top10['7']\n",
    "del top10['8']\n",
    "del top10['9']\n",
    "top10.to_csv(r'terms_frequency_nouns/'+\"top10.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
