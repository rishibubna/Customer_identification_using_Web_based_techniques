{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating functions to clean tweets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import lang_detect\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from langdetect import detect\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    return \" \".join(stems)\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def only_english(mystr):\n",
    "    if lang_detect(mystr)=='en':   #give up\n",
    "        return mystr\n",
    "    else :\n",
    "        return ''\n",
    "\n",
    "\n",
    "def clean_string(mystr):\n",
    "    mystr=mystr.lower()\n",
    "    mystr=re.sub(r\"\\\\\\w+\", \" \", mystr)\n",
    "    mystr=re.sub(r\"\\@\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"\\#\\w+\",\" \",mystr)\n",
    "    mystr=re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\" \",mystr)\n",
    "    \n",
    "    mystr=mystr[2:-1]\n",
    "    \n",
    "    cleantext = \"\".join([x.lower() if (x.isalpha() or x.isspace()) else ' ' for x in mystr])\n",
    "    return cleantext\n",
    "\n",
    "def remove_stop_words(mystr):\n",
    "    word_list= mystr.split()\n",
    "    cleaned_word_list=[w for w in word_list if w not in stop_words and len(w)>=2]\n",
    "    \n",
    "    return cleaned_word_list\n",
    "   \n",
    "    \n",
    "def join_sep(mylist):\n",
    "    return \" \".join(mylist)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impcolumns=list(vertical_stack.columns[np.argsort(chi2score[0])[::-1]][:1000])  #from a previous run, now i have cut those cells\n",
    "# #print(impcolumns)\n",
    "\n",
    "# with open(\"impcolumns.txt\", 'w') as f:\n",
    "#     for s in impcolumns:\n",
    "#         f.write(s + '\\n')\n",
    "\n",
    "#read these columns from a text file calledimpcolumns.txt if needed\n",
    "\n",
    "with open('impcolumns.txt', 'r') as f:\n",
    "    impcolumns = f.readlines()\n",
    "    \n",
    "\n",
    "    \n",
    "impcolumns = [line[:-1] for line in impcolumns]\n",
    "#print(impcolumns)\n",
    "vertical_stack=impcolumns\n",
    "\n",
    "import json\n",
    "\n",
    "with open('approach2_FORGRAPH1.json') as f:\n",
    "    data1 = json.load(f)\n",
    "\n",
    "with open('approach2_FORGRAPH2.json') as f:\n",
    "    data2 = json.load(f)\n",
    "    \n",
    "with open('approach4_final_data.json') as f:\n",
    "    data3 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading data and packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f= approach_FORGRAPH_tweets_data.json'\n",
    "import json\n",
    "with open('approach4_final_data.json', 'r') as fp:\n",
    "    tweet_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using oversampled data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading Classifier and prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename='DT_adaboost.pkl'\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    classifier = clf = joblib.load(file)\n",
    "\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new.to_csv('X_train_oversampled.csv', index = False, header=True)\n",
    "# y_new.to_csv('y_train_oversampled.csv', index = False, header=True)\n",
    "# X_test.to_csv('X_test.csv', index = False, header=True)\n",
    "# y_test.to_csv('y_test.csv', index = False, header=True)\n",
    "\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making tfidf from tweets and score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_score(user_tweets):     #here user_tweets would be a dict with key as the username and value as the tweets \n",
    "\n",
    "     \n",
    "    dataframes = {}\n",
    "    dataframes_empty = {}\n",
    "\n",
    "\n",
    "    for x in user_tweets:\n",
    "    #print(x)\n",
    "        \n",
    "    \n",
    "        try:\n",
    "            file=pd.DataFrame(user_tweets[x],columns=['id','text'])\n",
    "    \n",
    "            file['clean_text']=file['text'].apply(clean_string)\n",
    "            file['word_list']=file['clean_text'].apply(remove_stop_words)\n",
    "            file['word_list']=file['word_list'].apply(stem_and_lemmatize)\n",
    "            #file['word_list']=file['word_list'].apply(join_sep)\n",
    "            #file['word_list']=file['word_list'].apply(only_english)\n",
    "    \n",
    "            file=file.dropna()\n",
    "            file=file.loc[file['word_list']!='']\n",
    "            \n",
    "            ##############\n",
    "    \n",
    "    \n",
    "            vectorizer = TfidfVectorizer()\n",
    "        \n",
    "            tfidfmatrix = vectorizer.fit_transform(file['word_list'])\n",
    "            vocab = vectorizer.get_feature_names()\n",
    "\n",
    "            tfidf_data=tfidfmatrix.toarray()\n",
    "        \n",
    "            tfidf_pd=pd.DataFrame(data=tfidf_data,columns=vocab,index=file['id'])\n",
    "        \n",
    "            tfidf_pd = tfidf_pd.drop(tfidf_pd.columns.difference(impcolumns),1)\n",
    "       \n",
    "            dataframes[x] = tfidf_pd\n",
    "            #print(\"Success\",x)\n",
    "    \n",
    "        except ValueError as e: #happens when the vocabulary is empty\n",
    "        \n",
    "            #print(\"Failure\",x,file['text'])\n",
    "            dataframes_empty[x]='N/A'\n",
    "        \n",
    "        \n",
    "            continue\n",
    "            \n",
    "    tesla_followers=dataframes\n",
    "    feature_list=impcolumns\n",
    "    #UNIFORM PADDING\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "    \n",
    "        #print(len(test.columns))\n",
    "    \n",
    "\n",
    "        toadd=list(set(feature_list)-set(test.columns))\n",
    "\n",
    "\n",
    "        toadd_df=pd.DataFrame(np.zeros((test.shape[0], len(toadd))),columns=toadd)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        toadd_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        test=pd.concat([test,toadd_df],axis=1)\n",
    "    \n",
    "        tesla_followers[key]=test\n",
    "        #print(test.shape)\n",
    "    \n",
    "    #SCORE    \n",
    "    tesla_tweet_prediction=[]\n",
    "    score={}\n",
    "    for key in tesla_followers:\n",
    "        test=tesla_followers[key]\n",
    "        current_user_pred=classifier.predict(test)\n",
    "        tesla_tweet_prediction.append(current_user_pred)\n",
    "        score[key]=sum(current_user_pred)/len(current_user_pred)   \n",
    "    \n",
    "    \n",
    "    sorted_score = OrderedDict(sorted(score.items(), key=itemgetter(1),reverse=True))\n",
    "    sorted_score.update(dataframes_empty)    #adding users with No tweets\n",
    "    \n",
    "    return sorted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level={}\n",
    "first_no_of_tweets={}\n",
    "\n",
    "ctr=0\n",
    "\n",
    "for k, v in data1['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    first_level[k]=v['tweets']\n",
    "    first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "for k, v in data2['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    first_level[k]=v['tweets']\n",
    "    first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "for k, v in data3['tesla'].items():\n",
    "    #print(k)\n",
    "    #print(v.keys())\n",
    "    first_level[k]=v['tweets']\n",
    "    first_no_of_tweets[k]=len(v['tweets'])\n",
    "    \n",
    "    \n",
    "print(len(first_no_of_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#user_name_neg = ['AbbyChe88615215', 'QzI2jJcBFlZmomK', 'sonjahahn', 'Melectroner', 'barcemo2', 'Noahgreenberg12', 'AngelLadyVenus', 'Sagar59781359', 'timarajae', 'Moaz80334541', 'ppmax17', 'LOVE_Raelian', 'SchieblerXavier', 'nazwanazwanazwa', 'DevrajBelgaonk1', 'MatyalAzher', 'kingsugenius', 'greyeaglespouch', 'max_cardaci', 'klmgorov', 'rohitpandya31', 'ctominay', 'imsmruti', 'Bogdan40542498', 'Zeeshan01344758', 'AdamarisVelez', '1uEkvO6odBsDImj', 'MariannaGardar1', 'MelAlex97543906', 'valtomsgv', 'jackrow94480700', 'Domi9915', 'LGszHBzn6H9Aoec', 'QunNguy81644285', 'dblank64', 'MuhammadGhozy9', 'adedivision', 'Frank91806042', 'Seb57466842', 'HYnYang1999', 'KhengKouy', 'Saqlainsharif14', 'Raphael79724082', 'WisdomOgbonna12', 'karthikadr1', 'ibrahemka93', 'artemisia_annie', 'njtktm', 'BJ_Timson', 'MhHoussem', 'Owenchoi9', 'IdrisRockingidd', 'eHo9gixZa47kwcW', 'VickyGo13', 'IdreesAshrafMir', 'P4NT2', 'ICaldicott', 'gabi_emeli', 'LoRaine_fr', 'MarwaanWilliam1', 'StanWoodman4', 'FPrefekt', 'DevPate71188540', '7aDOxFkBKvyfl6', '__Nadeeka', 'priyankamoorthy', 'VivekPa63657629', 'dicksuck4money', 'shai_trakz', 'badal_khosla', 'SwapnilDurgeka1', 'barry_member', 'tsaharsharaf', 'SudipSi66921834', 'ColinCa42350619', 'JoeChen56203494', 'HapuNipuna', 'Whatsup65692095', 'mestevper4mance', 'AasimMushtaq10', 'CamilloGasparri', 'ZachFre06174485', 'Kien12092440', 'VRocchis', 'Charlie_WWright', 'Mathewsmutale9', 'Joshau72077726', 'benbetterb', 'indiana11011100', 'Pavan28843768', 'Andreaarman1', 'CoombesF', 'Soumy21134508', 'nadinelivgmail1', 'ChaudharyAvin', 'LoApa15', 'rachit_mangal', 'SooryanRajeev', 'AbhilashO8', 'aatrees', 'Muham83137969', 'RouelAndrew', 'ModiboDilika', 'LindaSouv', 'TommyTurkia', 'Griee4', 't_elevs', 'immeet6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(user_name_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(first_level))\n",
    "#c=0\n",
    "first_level_list= return_score(first_level)\n",
    "# for x in first_level:\n",
    "#     if x in user_name_neg:\n",
    "#         c+=1\n",
    "# print(c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_classification using text\n",
    "second_level={}\n",
    "for k, v in data1['tesla'].items():\n",
    "    second_level[k] = {}\n",
    "    follwer_list = []\n",
    "    user_tweets={}\n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                    follwer_list.append(i)\n",
    "                    if j['tweets']!=\"error\":\n",
    "                        user_tweets[i] = j['tweets']\n",
    "    second_level[k] = return_score(user_tweets)\n",
    "    \n",
    "for k, v in data2['tesla'].items():\n",
    "    second_level[k] = {}\n",
    "    follwer_list = []\n",
    "    user_tweets={}\n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                    follwer_list.append(i)\n",
    "                    if j['tweets']!=\"error\":\n",
    "                        user_tweets[i] = j['tweets']\n",
    "    second_level[k] = return_score(user_tweets)\n",
    "                        \n",
    "for k, v in data3['tesla'].items():\n",
    "    second_level[k] = {}\n",
    "    follwer_list = []\n",
    "    user_tweets={}\n",
    "    if isinstance(v, dict):\n",
    "        for i, j in v.items():\n",
    "            if not (i == \"tweets\" or i == 'location'):\n",
    "                    follwer_list.append(i)\n",
    "                    if j['tweets']!=\"error\":\n",
    "                        user_tweets[i] = j['tweets']\n",
    "    second_level[k] = return_score(user_tweets)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_score = {}\n",
    "\n",
    "for i in list(second_level.values()):\n",
    "    for (j,k) in i.items():\n",
    "            nodes_score[j] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (j,k) in return_score(first_level).items():\n",
    "        nodes_score[j] = k\n",
    "\n",
    "nodes_score['tesla'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording edges\n",
    "nodes = []\n",
    "edges=[]\n",
    "count = 0\n",
    "nodes.append('tesla')\n",
    "for i,j in data1['tesla'].items():\n",
    "    nodes.append(i)\n",
    "    tup_l1 = (\"tesla\",i)\n",
    "    edges.append(tup_l1)\n",
    "    count+= 1\n",
    "    if len(j.keys()) > 0:\n",
    "        a = list(j.keys())\n",
    "        a.remove('tweets')\n",
    "        a.remove('location')\n",
    "        for k in a:\n",
    "                count+= 1\n",
    "                tup = (i,k)\n",
    "                nodes.append(k)\n",
    "                edges.append(tup)\n",
    "\n",
    "for i,j in data2['tesla'].items():\n",
    "    nodes.append(i)\n",
    "    tup_l1 = (\"tesla\",i)\n",
    "    edges.append(tup_l1)\n",
    "    count+= 1\n",
    "    if len(j.keys()) > 0:\n",
    "        a = list(j.keys())\n",
    "        a.remove('tweets')\n",
    "        a.remove('location')\n",
    "        for k in a:\n",
    "                count+= 1\n",
    "                tup = (i,k)\n",
    "                nodes.append(k)\n",
    "                edges.append(tup)\n",
    "\n",
    "for i,j in data3['tesla'].items():\n",
    "    nodes.append(i)\n",
    "    tup_l1 = (\"tesla\",i)\n",
    "    edges.append(tup_l1)\n",
    "    count+= 1\n",
    "    if len(j.keys()) > 0:\n",
    "        a = list(j.keys())\n",
    "        a.remove('tweets')\n",
    "        a.remove('location')\n",
    "        for k in a:\n",
    "                count+= 1\n",
    "                tup = (i,k)\n",
    "                nodes.append(k)\n",
    "                edges.append(tup)\n",
    "print(count)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in train and test\n",
    "#node_based sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "unlabeled_nodes = [i for i in nodes_score if nodes_score[i] == 'N/A']\n",
    "\n",
    "for k in nodes:\n",
    "    if not(k in nodes_score.keys()):\n",
    "        unlabeled_nodes.append(k)\n",
    "\n",
    "nodes_data = list(nodes_score.keys())\n",
    "for i in unlabeled_nodes:\n",
    "    try:\n",
    "        nodes_data.remove(i)\n",
    "    except:\n",
    "        continue\n",
    "train_nodes, test_nodes = train_test_split(nodes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "nodes_score_copy = copy.deepcopy(nodes_score)\n",
    "\n",
    "for j in test_nodes:\n",
    "    unlabeled_nodes.append(j)\n",
    "\n",
    "for i in unlabeled_nodes:\n",
    "    nodes_score_copy[i] = threshold\n",
    "\n",
    "for k in range(50):\n",
    "    delta_change = 0\n",
    "    for l in unlabeled_nodes:\n",
    "        neigbours = []\n",
    "        for n,m in edges:\n",
    "            if n == l:\n",
    "                neigbours.append(m)\n",
    "            if m == l:\n",
    "                neigbours.append(n)\n",
    "        neigbour_scores = [nodes_score_copy[a] for a in neigbours]\n",
    "        old_val = nodes_score_copy[l]\n",
    "        nodes_score_copy[l] = np.mean(neigbour_scores)\n",
    "        #delta_change += abs(nodes_score_copy[l] - old_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = [1 if nodes_score_copy[i] > threshold else 0 for i in test_nodes]\n",
    "actual_values = [1 if nodes_score[i] > threshold else 0 for i in test_nodes]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(actual_values, predicted_values, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.2,0.3,0.4,0.5]\n",
    "[0.9585832619251643,0.851159793814433, 0.6146179401993355, 0.23239436619718312]\n",
    "\n",
    "import json\n",
    "with open('hierarchy_scores.json', 'w') as fp:\n",
    "    json.dump(second_level, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOMOPHILY\n",
    "one_one = 0\n",
    "zero_one = 0\n",
    "zero_zero = 0\n",
    "invalid = 0\n",
    "for i,j in edges:\n",
    "    if (i in nodes_score.keys()) and (j in nodes_score.keys()):\n",
    "        if nodes_score[i] >= 0.5 and nodes_score[j] >= 0.5:\n",
    "            one_one += 1\n",
    "        elif nodes_score[i] < 0.5 and nodes_score[j] >= 0.5:\n",
    "            zero_one += 1\n",
    "        elif nodes_score[i] >= 0.5 and nodes_score[j] < 0.5:\n",
    "            zero_one += 1\n",
    "        else:\n",
    "            zero_zero += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_one)\n",
    "print(zero_one)\n",
    "print(zero_zero)\n",
    "print(invalid)\n",
    "\n",
    "print(one_one+zero_one+zero_zero+invalid)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_population = []\n",
    "\n",
    "for _ in range(one_one+zero_zero):\n",
    "    edges_population.append(0)\n",
    "for _ in range(zero_one):\n",
    "    edges_population.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.choice(edges_population, 1000, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.bootstrap(samples, stat_func=bs_stats.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one/(one_one+zero_zero)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
